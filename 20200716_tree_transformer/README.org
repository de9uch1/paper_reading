# Copyright (c) Hiroyuki Deguchi.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

#+OPTIONS: toc:nil

#+TITLE: Tree Transformer: Integrating Tree Structures into Self-Attention
#+BEAMER_HEADER: \subtitle{(Wang et al., EMNLP 2019)}
#+AUTHOR: 出口 祥之 <\texttt{deguchi@ai.cs.ehime-u.ac.jp}>
#+DATE: 2020/07/16 二宮研 論文輪読会
#+BEAMER_HEADER: \institute{}
#+STARTUP: beamer
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [unicode, 12pt, aspectratio=43]

#+LATEX_HEADER: \usefonttheme{professionalfonts}
#+LATEX_HEADER: \usepackage[T1]{fontenc}

#+LATEX_HEADER: \usepackage[sort, compress]{natbib}
#+LATEX_HEADER: \let\realcitep\citep \renewcommand*{\citep}[1]{{\footnotesize\realcitep{#1}}}
#+LATEX_HEADER: \usepackage{url}

#+LATEX_HEADER: \usetheme{metropolis}
#+LATEX_HEADER: \setbeamertemplate{footline}{ \hfill \usebeamercolor[fg]{page number in head/foot} \usebeamerfont{page number in head/foot} \insertframenumber\kern1em\vskip2pt }
#+LATEX_HEADER: \setbeamertemplate{items}[default]
#+LATEX_HEADER: \setbeamertemplate{itemize item}{\small\raise0.5pt\hbox{$\blacksquare$}}
#+LATEX_HEADER: \setbeamertemplate{itemize subitem}{\footnotesize\raise1.5pt\hbox{$\bullet$}}
#+LATEX_HEADER: \setbeamertemplate{itemize subsubitem}{\scriptsize\raise1.5pt\hbox{$\blacktriangleright$}}
#+LATEX_HEADER: \setbeamertemplate{navigation symbols}{}
#+LATEX_HEADER: \usepackage{xltxtra}

#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage[absolute,overlay]{textpos}

#+LATEX_HEADER: \usepackage{pgfpages}
# #+LATEX_HEADER: \setbeameroption{show notes on second screen=right}

#+LATEX_HEADER: \XeTeXlinebreaklocale "ja"
#+LATEX_HEADER: \setsansfont{Noto Sans CJK JP}
#+LATEX_HEADER: \renewcommand{\baselinestretch}{1.3}

* Links
** Paper:
  - https://www.aclweb.org/anthology/D19-1098/
** Implementation:
  - https://github.com/yaushian/Tree-Transformer

* Introduction
- RNN では木構造を扱えるモデルが存在する (Tree-RNNs)
- Transformer ベースのモデルで直接木構造を扱うモデルはない

- シンプルなモジュールを追加するだけ
  - 実装が簡単
  - 教師なし構文解析で良い性能
  - 人間の直感とも一致するような階層構造が得られる
  - 説明可能な Attention を持つようになる

* Tree Transformer
#+ATTR_LATEX: :width \linewidth
[[./figure/Figure1.pdf]]

* Constituent Prior
** Attention 確率分布行列に Constituent Prior $C$ を掛け合わせる
:PROPERTIES:
:BEAMER_COL: 1.0
:BEAMER_ENV: block
:END:

- 通常の Transformer の確率分布行列 $E$
  \begin{equation*}
    E = \mathrm{softmax}(\frac{QK^\top}{\sqrt{d_k}})
  \end{equation*}

- Tree Transformer の確率分布行列 $E$
  \begin{equation*}
    E = C \odot \mathrm{softmax}(\frac{QK^\top}{\sqrt{d_k}})
  \end{equation*}
  ※ $\odot$ は要素積
  \vspace{0.2cm}
- $C$ は Constituent Attention モジュール (後述) により計算される

\begin{textblock*}{0.3\linewidth}(265pt, 130pt)
    \centering
    \includegraphics[width=\linewidth]{./figure/Figure1_b.pdf}
\end{textblock*}

* Constituent Attention
** $C$ を計算するモジュール
:PROPERTIES:
:BEAMER_COL: 0.8
:BEAMER_ENV: block
:END:
- $C_{i,j} (= C_{j, i})$ は単語 $w_i$ から単語 $w_j$ まで ($w_j$ から $w_i$ まで) が同じ構成要素である確率
- 隣り合った単語が同じ構成要素である結合確率 $a$ を後述の手法により計算 @@latex:\\@@ $a = \{a_1, \ldots, a_i, \ldots, a_N\}$
- $a$ から $C_{i,j} = \prod_{k=i}^{j-1} a_k$ を計算
  - $w_i$ 〜 $w_j$ 間の中の $a_{i \le k < j}$ の値が小さいときに $C_{i,j}$ も小さくなるよう，$C_{i, j}$ は和ではなく積で計算
  - 実際の計算では，勾配消失問題を回避するため $\mathrm{logsumexp}$ により計算

** figure
:PROPERTIES:
:BEAMER_COL: 0.25
:END:

#+ATTR_LATEX: :width 1.0\linewidth
[[./figure/Figure1_c.pdf]]

* Neighboring Attention
** 各レイヤで $a$ を計算
:PROPERTIES:
:BEAMER_COL: 1.1
:BEAMER_ENV: block
:END:
1. $w_i$ と $w_{i+1}$ をそれぞれ $d_{model}$ 次元の $q_i$ と $k_{i+1}$ に線形変換
2. スコア $s_{i, i+1} = \frac{q_i k_{i+1}}{d_{model} / 2}$ を計算 \vspace{0.3cm}
3. $p_{i, i+1}, p_{i, i-1} = \mathrm{softmax}(s_{i, i+1}, s_{i, i-1})$
   - \footnotesize $(p_{i, i+1} + p_{i, i-1}) = 1$ にしないと疎な分布にならないため重要 \normalsize
4. $p_{i, i+1}$ と $p_{i+1, i}$ の幾何平均より $\hat{a}_i$ を計算
   - $C$ を対称行列にするため
5. Hierarchical Constraint (後述) に $\hat{a}_i$ を渡して $a_i$ を計算

\begin{textblock*}{0.4\linewidth}(220pt, 110pt)
    \centering
    \includegraphics[width=\linewidth]{./figure/Figure2.pdf}
\end{textblock*}

* Hierarchical Constraint
\vspace{-2.5cm}
** レイヤ間の階層構造を構築
- 上のレイヤほど構成要素の幅を広くする
- レイヤ $l$ ，位置 $i$ の隣接単語の結合確率を $a_i^l$ とすると， $a_i^l = a_i^{l-1} + (1 - a_i^{l-1})\hat{a}_i^l$
  - なお， $a_i^{-1} = 0$
  - これにより制約 $a_i^{l-1} < a_i^l$ がかかる
- $a^l$ から $C^l$ を計算

\begin{textblock*}{0.45\linewidth}(190pt, 180pt)
    \centering
    \includegraphics[width=\linewidth]{./figure/Figure1_a.pdf}
\end{textblock*}

* \large Unsupervised Parsing from Tree Transformer
** Algorithm
:PROPERTIES:
:BEAMER_COL: 0.5
:END:
#+ATTR_LATEX: :width \linewidth
[[./figure/Algorithm1.pdf]]

** Tree
:PROPERTIES:
:BEAMER_COL: 0.6
:END:
#+ATTR_LATEX: :width \linewidth
[[./figure/Figure3.pdf]]

* Experiments
- モデルが木構造を捉えられるのか調べるため教師なし句構造解析により文法推論実験

#+ATTR_LATEX: :booktabs t
|------------+-----------------------------------|
| 訓練データ | WSJ                               |
| 訓練法     | Masked LM                         |
| 評価データ | Penn Treebank (WSJ-test / WSJ-10) |
|------------+-----------------------------------|

* Results
** F1 スコア (WSJ-test / WSJ-10)
*** WSJ-test
:PROPERTIES:
:BEAMER_COL: 0.5
:BEAMER_ENV: block
:END:
#+ATTR_LATEX: :width \linewidth
[[./figure/Table1.pdf]]

*** WSJ-10
:PROPERTIES:
:BEAMER_COL: 0.5
:BEAMER_ENV: block
:END:
#+ATTR_LATEX: :width \linewidth
[[./figure/Table2.pdf]]

* Results
** 構成要素の Recall (各ラベルで比較)
#+ATTR_LATEX: :width 0.5\linewidth
[[./figure/Table3.pdf]]

* Analysis
#+ATTR_LATEX: :width \linewidth
[[./figure/Figure4.pdf]]

* Interapretable Self-Attention
#+ATTR_LATEX: :width 0.8\linewidth
[[./figure/Figure5.pdf]]

* Masked Language Modeling
** \texttt{[MASK]} トークンの perplexity を評価
#+ATTR_LATEX: :width 0.5\linewidth
[[./figure/Table4.pdf]]
- perplexity は通常の Transformer ベースのモデルより Tree Transformer のほうが低い

* Limitations and Discussion
- 訓練済み BERT でパラメタ初期化を行うと性能が酷く低下
  - BERT の Attention が Tree Transformer と全く異なる構造を学習していることを示唆

- この 1 文がよくわからなかった
  - ``\textrm{In addition, with a well-trained Transformer, it is not necessary for the Constituency Attention module to induce reasonable tree structures, because the training loss decreases anyway.}''

* Conclusion and Future Work
- \textbf{Conclusion}
  - 木構造を Transformer に組み込む初めての試み
  - 提案手法の Constituent Attention により木構造を自動的に学習
    - 隣接する単語の結合確率から相互に結び付ける
  - 教師なし構文解析の性能は一貫した木構造を捉えるという点でモデルの有効性を示した

- \textbf{Future Work}
  - Transformer で木構造を捉える方向性について検討する価値はある
  - 解釈可能な Attention はモデルが自然言語を処理する方法を説明し，将来のさらなる改善を導く
