# Copyright (c) Hiroyuki Deguchi.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

#+TITLE: Dynamic Programming Encoding for Subword Segmentation in Neural Machine Translation
#+SUBTITLE: (He et al., ACL 2020) 
#+AUTHOR: 出口 @@latex:~@@ 祥之 @@latex:\\ \lower2.0pt\hbox{\materials} \texttt{deguchi@ai.cs.ehime-u.ac.jp}@@
#+DATE: 2020/09/11 @@latex:~@@ 第 2 回 NLG/MT Reading Group
#+BEAMER_HEADER: \institute{}
#+STARTUP: beamer
#+LATEX_CLASS: beamer
#+LATEX_CLASS_OPTIONS: [unicode, 12pt, xdvipdfmx, aspectratio=43]
#+OPTIONS: H:2 toc:nil

#+LATEX_HEADER: \usepackage[backend=bibtex, style=authoryear, maxcitenames=2]{biblatex}
# #+LATEX_HEADER: \AtEveryCitekey{\iffootnote{\tiny\reffont}{\color{blue}}}
#+LATEX_HEADER: \addbibresource{../resources/anthology.bib}
#+LATEX_HEADER: \addbibresource{../resources/my.bib}
#+LATEX_HEADER: \let\oldcite\cite
#+LATEX_HEADER: \renewcommand{\cite}[1]{{\scriptsize\reffont{(\oldcite{#1})}}}
#+LATEX_HEADER: \newcommand{\citet}[2][\footnotesize]{{\reffont#1\citeauthor*{#2} (\citeyear{#2})}}
#+LATEX_HEADER: \newcommand{\mycite}[1]{{\scriptsize\reffont({\citeauthor*{#1}, \citeyear{#1}})}}
#+LATEX_HEADER: \newcommand{\myfootcite}[1]{\footnote{\tiny\reffont\citetitle{#1}, \citeauthor*{#1}, \citeyear{#1}.}}
#+LATEX_HEADER: \usepackage{url}

#+LATEX_HEADER: \usetheme[numbering=fraction]{metropolis}
#+LATEX_HEADER: \setbeamertemplate{items}[default]
#+LATEX_HEADER: \setbeamertemplate{itemize item}{\small\raise0.5pt\hbox{$\blacksquare$}}
#+LATEX_HEADER: \setbeamertemplate{itemize subitem}{\footnotesize\raise1.5pt\hbox{$\bullet$}}
#+LATEX_HEADER: \setbeamertemplate{itemize subsubitem}{\scriptsize\raise1.5pt\hbox{$\blacktriangleright$}}
#+LATEX_HEADER: \setbeamertemplate{enumerate item}{\textbf{(\arabic{enumi})}}
#+LATEX_HEADER: \addtolength{\skip\footins}{6pc plus 10pt}
#+LATEX_HEADER: \usepackage{xltxtra}

#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage[absolute,overlay]{textpos}

#+LATEX_HEADER: \usepackage{pgfpages}
# #+LATEX_HEADER: \setbeameroption{show notes on second screen=right}

#+LATEX_HEADER: \usefonttheme{professionalfonts}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{fontspec}
#+LATEX_HEADER: \XeTeXlinebreaklocale "ja"
#+LATEX_HEADER: \usepackage{xeCJK}
# #+LATEX_HEADER: \setsansfont[AutoFakeSlant=0.2]{Noto Sans CJK JP}
#+LATEX_HEADER: \setsansfont[BoldFont={Fira Sans Bold}]{Fira Sans Book}
#+LATEX_HEADER: \setCJKmainfont{Noto Sans CJK JP}
#+LATEX_HEADER: \setCJKsansfont{Noto Sans CJK JP}
#+LATEX_HEADER: \setCJKromanfont{Noto Serif CJK JP}
#+LATEX_HEADER: \xeCJKDeclareCharClass{CJK}{`※}
# #+LATEX_HEADER: \setromanfont[AutoFakeSlant=0.2]{Noto Serif CJK JP}
#+LATEX_HEADER: \newfontfamily\firasans{Fira Sans}
#+LATEX_HEADER: \newfontfamily\emojifont{Noto Emoji}
#+LATEX_HEADER: \newfontfamily\octicons{github-octicons}
#+LATEX_HEADER: \newfontfamily\materials{Material Icons}
#+LATEX_HEADER: \newfontfamily\reffont{Times New Roman}
#+LATEX_HEADER: \renewcommand{\baselinestretch}{1.3}

** \hbox{\octicons} Links
*** \raise0.5pt\hbox{\octicons} Paper
**** https://www.aclweb.org/anthology/2020.acl-main.275/
*** \raise0.5pt\hbox{\octicons} Source Code
**** https://github.com/xlhex/dpe

** Introduction
\vspace{-0.2cm}
*** 動的計画法を用いた新たなサブワード分割法を提案
\vspace{-0.2cm}
\small
    - 目的言語文の分割を潜在変数と見做し，周辺化 \vspace{-0.2cm}
    - ``\alert{Mixed character-subword Transformer}'': @@latex:\\@@ 原言語文が与えられたときの目的言語文の分割を獲得
*** 
\vspace{-1.0cm}
\metroset{block=fill}
**** NMT におけるサブワード分割
    :PROPERTIES:
    :BEAMER_ENV: block
    :END:
\vspace{-0.4cm}
\small
#+ATTR_LATEX: :align rl
| \textbf{貪欲法:}             | バイトペア符号化 (BPE)\myfootcite{sennrich-etal-2016-neural}, WordPiece\myfootcite{schuster-nakajima-2012-japanese} |
| \textbf{確率的アルゴリズム:} | ユニグラムLM\myfootcite{kudo-2018-subword}, BPE-dropout\myfootcite{provilkov-etal-2020-bpe}                         |
| \textbf{動的計画法:}         | 本論文の提案手法 \vspace{-0.3cm}                                                                                    |

*** 
    :PROPERTIES:
    :BEAMER_ENV: ignoreheading
    :END:
    \vspace{0.3cm}

** Related Work (Greedy Segmentation)
*** BPE \cite{sennrich-etal-2016-neural}, WordPiece \cite{schuster-nakajima-2012-japanese}
  - 隣接する頻出サブワードから順に，予め指定した @@latex:\\@@ 語彙数に到達するまで再帰的に結合 (BPE)
  - 語彙数とデコード速度はトレードオフ
    - (語彙数を小さくするだけであれば文字単位でよい)
    - テキスト圧縮の技術を利用
    - 語彙数の上限を制約とし，文長が短くなるような @@latex:\\@@ 分割を得るアルゴリズム

  - 例: :: \textrm{unconscious → un + conscious}

** Related Work (Stochastic Segmentation)
*** ユニグラムLM \cite{kudo-2018-subword}, BPE-dropout \cite{provilkov-etal-2020-bpe}
 - \normalsize 複数分割候補を得られる
   - ユニグラムLM: 尤度ベースでサンプリング
   - BPE-dropout: BPE 結合時に確率的に棄却
   - NMT 訓練時に分割を確率的に得ることでデータ拡張 \textrm{(Data Augumentation)} \sffamily の効果
     - モデルの頑健性向上

 - 例: :: @@latex:\textrm{unconscious → \{un + concious, uncon + scious\}}@@


** \normalsize Related Work (Dynamic Programming Algorithms)
\small
*** 音声認識 \cite{wang-etal-2017-sequence}
    - 取り得る全ての分割や入出力間のアライメントの確率を動的計画法により計算

*** 非自己回帰 NMT モデル \cite{chan-etal-2020-imputer, saharia-etal-2020-nonautoregressive}
    - \textbf{Imputer} \cite{chan-etal-2020-imputer} : @@latex:\\@@ Connectionist Temporal Classification (CTC) を用い，定数回のデコードで出力とその順序を予測
    - 非自己回帰 NMT モデルに Imputer を適用 \cite{saharia-etal-2020-nonautoregressive}

* Proposed Method
** Latent Subword Segmentation - Definitions
*** \hspace{-0.75cm}目的言語文の分割を潜在変数とみなす
**** 
     :PROPERTIES:
     :BEAMER_COL: 1.0
     :END:
 - $M$ 個のサブワード: $\{\boldsymbol{y}_{z_i, z_{i+1}}\}_{i=1}^M$
   - $\boldsymbol{y} = (y_1, \ldots, y_T)$: 目的言語文の文字列 
   - $\boldsymbol{z} = (z_1 ,\ldots, z_{M+1})$: 境界位置系列
     - $0 = z_1 < z_2 < \ldots < z_M < z_{M+1} = T$ (昇順)
   - $\boldsymbol{y}_{a,b}$: $(a+1)^\text{th}$ から $b^\text{th}$ まで結合したサブワード

**** 
     :PROPERTIES:
     :BEAMER_COL: 0.42
     :END:
 \begin{textblock*}{\linewidth}(230pt, 40pt)
     \centering
     \includegraphics[width=\linewidth]{./figure/Figure1.pdf}
 \end{textblock*}

*** 例:
    :PROPERTIES:
    :BEAMER_COL: 0.5
    :BEAMER_ENV: block
    :END:
 \footnotesize
 - 辞書 $\mathcal{V} = \{\text{c, a, t, ca, at}\}$
 - 目的言語文 $\boldsymbol{y} = \text{cat}$

*** 
    :PROPERTIES:
    :BEAMER_COL: 0.5
    :END:
 \footnotesize

 #+ATTR_LATEX: :booktabs t
 |------------------+-----------------------|
 | $\boldsymbol{z}$ | サブワード列          |
 |------------------+-----------------------|
 | $(0,1,3)$        | (c, at)               |
 | $(0,2,3)$        | (ca, t)               |
 | $(0,1,2,3)$      | (c, a, t)             |
 |------------------+-----------------------|

** Latent Subword Segmentation - Likelihood
*** 連鎖律を用いてサブワード列の対数尤度を表現
 - 各サブワード位置において語彙の確率分布を生成
   \begin{equation*}
     \log p(\boldsymbol{y},\boldsymbol{z} | \boldsymbol{x}) = \sum_{i=1}^{|\boldsymbol{z}|} \log p(\boldsymbol{y}_{z_i, z_{i+1}} | \boldsymbol{y}_{z_1, z_2},\ldots,\boldsymbol{y}_{z_{i-1}, z_i}, \boldsymbol{x})
   \end{equation*}
   ※ $\boldsymbol{x}$ : 原言語文
 - 殆どの NMT では $\boldsymbol{z}$ を暗黙的に $\log p(\boldsymbol{y}, \boldsymbol{z}) \approx \log p(\boldsymbol{y})$ と仮定

** \normalsize Latent Subword Segmentation - Latent Variable
*** $\boldsymbol{z} \in \mathcal{Z}_{y} (\boldsymbol{y} \text{の分割集合})$ を潜在表現とみなす
 - $p(\boldsymbol{y} | \boldsymbol{x}) = \sum_{\boldsymbol{z}} p(\boldsymbol{y}, \boldsymbol{z} | \boldsymbol{x})$ とする
   \begin{equation*}
     \small \log p(\boldsymbol{y} | \boldsymbol{x}) = \log\sum_{\boldsymbol{z}\in\mathcal{Z}_y}\exp\sum_{i=1}^{|\boldsymbol{z}|} \log p(\boldsymbol{y}_{z_i, z_{i+1}} | \boldsymbol{y}_{z_1, z_2},\ldots,\boldsymbol{y}_{z_{i-1}, z_i}, \boldsymbol{x})
   \end{equation*}
   ※ 対数周辺尤度の下限: $\log p(\boldsymbol{y} | \boldsymbol{x}) \ge \log p(\boldsymbol{y}, \boldsymbol{z} | \boldsymbol{x})$

 - 各サブワードの確率が条件部のコンテキストの分割に依存するため，巨大な空間 $\mathcal{Z}_y$ 上での厳密な周辺化は組み合わせ爆発を起こす
   - コンテキストが次に来るサブワードの確率に @@latex:\\@@ 影響しないモデルが必要

** A Mixed Character-Subword Transformer
*** 文字に基づいてサブワードを生成する Transformer
 - 条件部のコンテキストを文字のみに
   \begin{equation*}
     \log p(\boldsymbol{y}, \boldsymbol{z} | \boldsymbol{x}) = \sum_{i=1}^{|\boldsymbol{z}|} \log p(\boldsymbol{y}_{z_i, z_{i+1}} | y_{z_1}, \ldots, y_{z_i}, \boldsymbol{x})
   \end{equation*}

 - $\boldsymbol{y}$ の各文字位置 $t$ において，次に来るサブワード $w \in \mathcal{V}$ の分布を以下に基づいて生成
   \begin{equation*}
     p(w | y_{1}, \ldots, y_{t}, \boldsymbol{x}) = \frac{\exp(f(y_1,\ldots,y_t)^\top e(w))}{\sum_{w' \in \mathcal{V}}\exp(f(y_1,\ldots,y_t)^\top e(w'))}
   \end{equation*}
   - \vspace{-0.5cm} $f(\cdot)$ : Transformer により条件部の計算
   - $e(\cdot)$ : ソフトマックス層の重み

** A Mixed Character-Subword Transformer
*** $t$ ステップ目のモデル出力
    :PROPERTIES:
    :BEAMER_COL: 0.65
    :BEAMER_ENV: block
    :END:
 1. $t$ ステップ目でサブワード $w$ を生成
 2. サブワード $w$ の文字をデコーダに入力 ( $t+1$ から $t+|w|$ まで )
 3. $t+|w|$ ステップ目で次のサブワードを生成

*** 
    :PROPERTIES:
    :BEAMER_COL: 0.35
    :END:
 #+ATTR_LATEX: :width 1.1\linewidth
 [[./figure/Figure2.pdf]]

** Optimization
*** 目的関数 $\mathcal{L}(\theta)$ を最大化
   \begin{equation*}
     \mathcal{L}(\theta) = \sum_{(\boldsymbol{x}, \boldsymbol{y}) \in \mathcal{D}} \log P_\theta (\boldsymbol{y} | \boldsymbol{x})
   \end{equation*}

*** 必要な計算
   - 周辺尤度の計算
   - 対数周辺尤度の勾配計算

** Exact Marginalization
*** 動的計画法を用いて周辺尤度を計算
 - サブワードの出力確率が文字のみによって得られるため動的計画法によって対数周辺尤度が計算可能
\vspace{-0.5cm}

#+ATTR_LATEX: :width 1.05\linewidth
[[./figure/Algorithm1.pdf]]
\vspace{-0.5cm}
 - 計算量:  $\mathcal{O}(mT)$
   - $m$ : 語彙に含まれる最長の単語の文字数

** Gradient Computation
*** 計算量に関する問題点
 - 通常の Transformer デコーダより 8 倍遅く，メモリ使用量も増加 @@latex:\footnote{\texttt{PyTorch}での著者実装で比較}@@
   - DP アルゴリズムと文字レベルでの演算による系列長の増加が原因

*** 対処法
 - Transformer のレイヤ数を 6 から 4 に削減
 - 16 ステップ分勾配蓄積 \textrm{(Gradient Accumulation)} してからパラメタ更新
\vspace{0.5cm}

** Segmenting Target Sentences
*** Dynamic Programming Encoding (DPE): @@latex:\\@@ 最大事後確率を持つ目的言語文の分割を探索
 #+ATTR_LATEX: :width \linewidth
 [[./figure/Algorithm2.pdf]]

** Segmenting Target Sentences
 - Mixed character-subword Transformer は @@latex:\\@@ \alert{訓練データ}の\alert{目的言語文}の分割のためのみに使用
 - 分割した文で通常のサブワード Transformer を訓練
 #+ATTR_LATEX: :width 0.5\linewidth
 [[./figure/Figure3.pdf]]

** Experiments
 - データセット :: WMT09 En-Hu, WMT14 En-De, WMT15 En-Fi, WMT16 En-Ro, WMT18 En-Et
 - モデル ::
 #+ATTR_LATEX: :booktabs t
 |--------------------+------------------------|
 | NMT アーキテクチャ | Transformer base       |
 | 分割 (原言語側)    | BPE-dropout $(p=0.05)$ |
 | 　　 (目的言語側)  | DPE (提案手法)         |
 |--------------------+------------------------|

** Main Results
 #+ATTR_LATEX: :width 1.05\linewidth
 [[./figure/Table2.pdf]]

** Segmentation Examples
 #+ATTR_LATEX: :width 1.05\linewidth
 [[./figure/Table3.pdf]]
 - 他の例は論文参照

** Conditional Subword Segmentation
*** 原言語文を条件部に入れず，LM で分割
\vspace{-0.3cm}
 #+ATTR_LATEX: :width 0.5\linewidth
 [[./figure/Table5.pdf]]

\vspace{-0.5cm}
*** 同一の目的言語文で原言語側を変えて違いを比較
\vspace{-0.3cm}
 #+ATTR_LATEX: :width 0.5\linewidth
 [[./figure/Figure4.pdf]]

** Conditional Subword Segmentation
*** \small 原言語文が BPE-dropout によって変化することの有効性
 #+ATTR_LATEX: :width 0.6\linewidth
 [[./figure/Table6.pdf]]

** DPE vs BPE
*** 目的言語側の分割アルゴリズムを変えて比較
 #+ATTR_LATEX: :width 0.6\linewidth
 [[./figure/Table7.pdf]]

** Conclusion
*** 新たなサブワード分割法 @@latex:\\@@ Dynamic Programming Encoding を提案
  - \alert{Mixed character-subword Transformer} により @@latex:\\@@ 目的言語文を分割
   - 目的言語文の分割を潜在変数と見做して周辺化
   - 条件部のコンテキストを文字にすることで @@latex:\\@@ 動的計画法が適用可能に
   - 分割時は事後確率が最大となる分割を出力
 - BPE だけでなく BPE-dropout と比較しても @@latex:\\@@ 翻訳性能が向上
